{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "# Palmer Penguins: a machine learning example\n",
    "\n",
    "In this notebook we are going to cover:\n",
    "\n",
    "- [Data Science and Machine Learning](#ds)\n",
    "- [Meet the Penguins](#about) \n",
    "- [Scikit-learn](#sl) \n",
    "- [Loading the data](#load) \n",
    "- [Preparing the data for sklearn](#prep) \n",
    "- [Model creation & evaluation](#mce) \n",
    "- [Model visualisation](#mv) \n",
    "- [Choosing a different model](#ms) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "<a id='ds'></a>\n",
    "\n",
    "## Data Science and Machine Learning\n",
    "\n",
    "<font color='darkblue'>*A computer program is said to learn from experience **E** with respect to some set of tasks **T** and performance measure P if its performance at tasks in **T**, as measured by **P**, improves with experience **E**.*</font>\n",
    "\n",
    "Tom M. Mitchell (prominent Machine Learning researcher)\n",
    "\n",
    "- **T** = determine the species of a penguin\n",
    "- **E** = previously collected data from penguins *(flipper length, body mass, bill length and bill depth)*\n",
    "- **P** = a performance metric (e.g. accuracy) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='about'></a>\n",
    "\n",
    "## Meet the Palmer Penguins!\n",
    "\n",
    "\n",
    "The data was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antartica LTER. The goal of the dataset is to provide a great dataset for data exploration, visualisation and - in this case - a demonstration of the scikit-learn API. \n",
    "\n",
    "![](https://raw.githubusercontent.com/STATS250SBI/palmerpenguins/master/man/figures/lter_penguins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/bill-dimensions.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "### Classification\n",
    "\n",
    "We're going to look using the following features to predict the species of penguin:\n",
    "\n",
    "- Bill length (mm) \n",
    "- Bill depth (mm)\n",
    "- Flipper length (mm) \n",
    "- Body mass (grams) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sl'></a>\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/1200px-Scikit_learn_logo_small.svg.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "\n",
    "\n",
    "## Scikit-Learn\n",
    "Scikit-learn is *the* library for machine learning in Python, often considered the swiss army knife of machine learning. \n",
    "\n",
    "\n",
    "#### Why scikit-learn?\n",
    "- Many available machine learning models\n",
    "- Models are implemented by an expert team and checked by a large community\n",
    "- Consistent API for wide variety of algorithms\n",
    "- Covers most machine-learning tasks\n",
    "- Commitment to documentation, consistency and usability\n",
    "- Designed to work with other key Python libraries (NumPy, Pandas etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "\n",
    "## Loading our data\n",
    "\n",
    "There are many places your data can originate from. Maybe you want to load it from a Excel file you have stored locally on your system, maybe you have a .csv file stored online somewhere. Scikit-learn comes with various standard datasets that can be used for practice, that can be loaded if you have scikit-learn installed on your system. \n",
    "\n",
    "However, the dataset we will be using today (the Palmer penguins dataset) does not come from scikit-learn, but from a visualisation package called `seaborn`. A dataset loaded from seaborn will be a Pandas dataframe and can be used as such. Pandas is a powerful library for data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins')\n",
    "penguins.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['island'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep'></a>\n",
    "\n",
    "## Preparing the data for scikit-learn\n",
    "\n",
    "Scikit-learn estimators assume that all values in an array are numerical, and that all have and hold meaning.\n",
    "\n",
    "**Missing values:**\n",
    "Some data point entries that have no value which scikit-learn so we must find a way to deal with the missing values. \n",
    "\n",
    "**Dropped unwanted fields**\n",
    "Some features (sex of the penguin and the island where the penguin was spotted) are not numerical. Since we are not interested in these, we will drop them.\n",
    "\n",
    "### <mark>Activty</mark>\n",
    "\n",
    "Drop the missing values and the unwanted columns. Overwrite the penguins dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint:***\n",
    "\n",
    "|Syntax|\n",
    "|:---|\n",
    "|`df.dropna()`|\n",
    "|`df.drop(columns, axis)`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/prep-sklean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Feature Matrix and Target Vector\n",
    "\n",
    "We use our knowledge of Pandas to create:\n",
    "\n",
    "|**Feature matrix X**|**Target vector y**|\n",
    "|:---:|:---:|\n",
    "|Describes the data. |Describes the output. |\n",
    "|The attributes to base predictions on. |The label that you want to predict. |\n",
    "|Shape N x M (number of samples x number of features)|Shape N (one output for every data point)|\n",
    "|Example: for each penguin, we have bill length, bill depth, body mass and flipper length. The shape of the feature matrix would be N (number of penguins) x 4.|Example: if we have recorded the features of 200 penguins, our y would contain the corresponding species of those 200 penguins (3 different options). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = penguins.drop('species', axis=1)\n",
    "y = penguins['species']\n",
    "\n",
    "print(f'The shape of feature matrix X is: {X.shape}')\n",
    "print(f'The shape of target vector y is: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "An important goal of machine learning is to create a model that does not only do well on the data that it has already seen, but will also perform well under new circumstances on data that is has not seen before. We call this _generalization_. \n",
    "\n",
    "Imagine this: Penguin A is a gentoo (bill length of 33, bill depth of of 16, flipper length of 180 and body mass of 3500 grams).   Penguin A was presented during the training of our model; that means, penguin A was one of the examples that the algorithm used to create an understanding of what a gentoo looks like and how you can distinguish it from a chinstrap or adÃ©lie. \n",
    "\n",
    "If we want to know how well our model does, asking the model to classify our penguin A does not give us a lot of information. Even if the model is correct, do we know whether it has really truly learned the relationship between the features and the targets (ie. flipper length of >X is always species Y), or has it simply memorized the original data and does it recognise penguin A from the training phase? \n",
    "\n",
    "That's why we want to separate our dataset into two parts:\n",
    "* The _training_ set: this is the data (features and targets) that will guide the learning process. \n",
    "* The _test_ set: this is the data (features and targets) that we will use to _evaluate_ how well our model has learned. \n",
    "\n",
    "Scikit-learn's `train_test_split` function allows us to split the data in a train- and testset. By default, the test set size is set to 25% and the data is shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "print(f'The size of our feature matrix for the train set is: {X_train.shape}')\n",
    "print(f'The size of our target vector for the train set is: {y_train.shape}')\n",
    "\n",
    "print(f'\\nThe size of our feature matrix for the test set is: {X_test.shape}')\n",
    "print(f'The size of our target vector for the test set is: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our data is in fact shuffled: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mce'></a>\n",
    "\n",
    "## Model creation and evaluation\n",
    "\n",
    "Now we're ready to create our machine learning model! \n",
    "\n",
    "Scikit-learn has a rich collection of algorithms readily available. Depending on the case you are working on, scikit-learn most likely has a model that will suit your purposes. \n",
    "\n",
    "#### Scikit-Learn API usage steps when training a model\n",
    "1. Choosing a model class and importing that model \n",
    "2. Choosing the model hyperparameters by instantiating this class with desired values.\n",
    "3. Training the model to the preprocessed train data by calling the `fit()` method of the model instance.\n",
    "4. Evaluating model's performance using available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import the chosen algorithm \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: instantiate the model with the chosen hyperparameters\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: train the model with the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now trained a model that can be used to make predictions on new data. Remember our test set? That's new, unseen data to the model that we can now create predictions on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these predictions against our original data to see how well our model does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we don't have to do that comparison ourselves. Scikit-learn has made many implementations of possible metrics readily available, such as accuracy. \n",
    "\n",
    "$\\text{accuracy} = \\frac{correct}{total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! \n",
    "\n",
    "But accuracy is not the only metric you could be interested in. Alternatives are, for example, _precision_ and _recall_. \n",
    "\n",
    "* _Precision_ is the proportion of positive identifications that was actually correct. \n",
    "* _Recall_ is the proportion of actual positives that was identified correctly.\n",
    "* _F1 score_ is a function of precision and recall, that you use when you seek a balance between precision and recall. \n",
    "\n",
    "In some cases, precision is more important. For YouTube's recommendation system for example: you won't be able to show _ALL_ relevant videos, but it is important that the ones you do show _are_ relevant. \n",
    "\n",
    "However, in medical context, _recall_ is often more important. After all, if we mistakingly tell a person with cancer that they're healthy, that can have more severe consequences than the other way around. \n",
    "\n",
    "Precision, recall and F1 are all available with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_pred, y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mv'></a>\n",
    "\n",
    "## Model Visualisation\n",
    "\n",
    "One of the advantages of decision trees over some of the other available models, is that decision trees are relatively easy to interpret. By visualising the tree-like structure of the decision tree, we can understand why the model classifies samples the way it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "plot_tree(model, \n",
    "          ax=ax, \n",
    "          feature_names = X.columns, \n",
    "          class_names = y.unique());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ms'></a>\n",
    "\n",
    "## Choosing a different model \n",
    "\n",
    "What happens when we're interested in a model other than the decision tree? \n",
    "\n",
    "That's actually really easy. You simply replace the chosen model with another and the rest of the pipeline can stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Uncomment the model that you want to try\n",
    "# model = DecisionTreeClassifier()\n",
    "# model = RandomForestClassifier()\n",
    "# model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f'Model accuracy: {model.score(X_test, y_test)}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "# Summary\n",
    "\n",
    "Scikit-learn is an excellent, resourceful tool for machine learning in Python. \n",
    "\n",
    "We've seen how we can split a dataset with `train_test_split` into a train and test set, create and train a model, use the trained model to create predictions, and how to use the tools from `sklearn.metrics` to evaluate how good the model is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Want to learn more? Join us on a public course:**\n",
    "- [Python for Data Analysts](https://godatadriven.com/training/python-for-data-analysts-training/)\n",
    "- [Certified Python for Data Science](https://godatadriven.com/training/data-science-python-foundation-training/)\n",
    "- [And more!](https://godatadriven.com/what-we-do/train/#upcoming)\n",
    "\n",
    "Interested in our other courses? Download our [Training Guide](https://godatadriven.com/topic/training-brochure/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src='images/download.png' width='80px' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you would like to <mark>save this notebook</mark> there is a Download button at the top of the page. This will download the `.ipynb`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not planning to get Anaconda but you want to save the work you've done, got to `File -> Download as` and choose `.html`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/visit-repo.png' width='60px' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can click Visit repo at the top to navigate to the github repo where you can download everything as a `.zip` file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
