{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "# Feature Engineering: An example with bikes\n",
    "\n",
    "The success of a machine learning algorithm is partly dependent on how you choose to represent the data.  You may have seen this before in practice with **preprocessing techniques** like *one hot encoding your categorical data*, data cleaning (removing or imputing *missing data*), and the effectivness of *scaling/normalizing* your data. \n",
    "\n",
    "Feature engineering is the **practice of creating new features** from your **existing** data or **additional** data sources to improve model performance. In this notebook, we will show how to create new features that might improve model performance and discuss common feature engineering practices:\n",
    "\n",
    "- [About the data](#data)\n",
    "- [Baseline model](#baseline)\n",
    "- Feature Engineering\n",
    "    - [Date Features](#date)\n",
    "    - [Inferred Features (from past)](#past)\n",
    "    - [Encoding](#encoding)\n",
    "- [Types of Feature Engineering](#types)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Next Steps](#next)\n",
    "\n",
    "Before we do anything let's load in the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from holidays import HOLIDAYS\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "\n",
    "## About the data\n",
    "\n",
    "This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in the [Capital bikeshare](https://www.capitalbikeshare.com/) system. It also includes the corresponding weather and seasonal information. Our target variable is `cnt` - the number of bikes rented out on a particular day.\n",
    "\n",
    "<img src='images/bikes.png' width=500px>\n",
    "\n",
    "Let's read it in and take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('data/bike-rental.csv', parse_dates = ['datetime'], index_col='datetime')\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise: Let's get to know this data!</mark>\n",
    "\n",
    "Answer the questions below.\n",
    "\n",
    "Here are some methods that might be useful (remember to replace `df` with `bike`!):\n",
    "\n",
    "1. `df.info()`, `df.isnull()`\n",
    "2. `df.describe()`, `df.mean()`\n",
    "3. `df.groupby(df.index.year).mean()`\n",
    "4. `df['column'].plot()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <mark>How many missing values are there in the data?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <mark>What is the earliest and latest date (min/max) in the data?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. <mark>Which year was the most humid on average (mean)?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <mark>Plot the amount of bikes (`cnt`) (Extra: by month) - how does the amount of bikes being rented change between 2011 and 2012?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/explore-data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "\n",
    "## Baseline model\n",
    "\n",
    "The datetime format as it is currently presented is great for an index, however it is not usable for our sklearn pipeline. We will add data to represent separately the year, month, day and hour (minutes and seconds not recorded as per the dataset description). \n",
    "\n",
    "<img src='https://i.pinimg.com/originals/18/cc/16/18cc16fe8b55022d116a8eb937f3e8ec.jpg' width='150px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "### Pandas Pipelines: \n",
    "\n",
    "In pandas we can use the `.pipe()` method to modularise our code and create clean code! \n",
    "\n",
    "We do this by creating functions that section our code. Each function takes in a dataframe as an argument; we then use this function with the pipe method to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_values(df):\n",
    "    \"\"\" Preprocessing function\n",
    "    Creates year, month, day, hour columns.\n",
    "    \"\"\"\n",
    "    df = df.assign(**{'year': df.index.year,\n",
    "                      'month': df.index.month,\n",
    "                      'day': df.index.day,\n",
    "                      'hour': df.index.hour})\n",
    "    \n",
    "    return df\n",
    "\n",
    "bikes_processed = bikes.pipe(get_date_values)\n",
    "bikes_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have converted our dates! This means we're ready for our first pipeline. The below function builds a model running through all the steps of scikit learn:\n",
    "\n",
    "***Scikit Learn Steps***\n",
    "\n",
    "1. Split data into X and y (features & target)\n",
    "2. Encode the features in X where necessary (eg. create dummies)\n",
    "3. Create train test split (without shuffling so the holdout is always the tail end of the data)\n",
    "4. Build the model using a pipeline and specified model algorithm\n",
    "5. Evaluate the model using Scikit Learn metrics (in our case r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xy(df, target_feature='cnt', onehotencoding = None):\n",
    "    \n",
    "    # create dummy features\n",
    "    \n",
    "    if onehotencoding:\n",
    "        df = (\n",
    "            df\n",
    "            .join([pd.get_dummies(df[col]) for col in onehotencoding])\n",
    "            .drop(onehotencoding, axis=1)\n",
    "            .rename(str.lower, axis=1)\n",
    "        )\n",
    "    \n",
    "    # split the independent features and the target into X and y\n",
    "    \n",
    "    X = df.drop([target_feature], axis=1)\n",
    "    y = df.loc[:, target_feature]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_model(X, y, model, onehotcolumns=None, target_feature='cnt'):     \n",
    "    \"\"\" Trains model\n",
    "    \n",
    "    X: pandas DataFrame of the features\n",
    "    y: pandas Series of the target variable\n",
    "    train_index: list of indices of X/y which belong to the train set\n",
    "    test_index: list of indices of X/y which belong to the test set\n",
    "    model: sklearn model\n",
    "    \"\"\"\n",
    "    \n",
    "    # create train-test set split. \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False)\n",
    "        \n",
    "    # train the model. \n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate the model. \n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    score_train = r2_score(y_train, y_train_pred)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    print(f'Building model: {model}'.split('(')[0])\n",
    "    print(f'Train score: {score_train.round(2)}')\n",
    "    print(f'Test score: {score.round(2)}\\n')\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our steps are defined! Time to initlialise our models, process our data and train the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestRegressor(max_depth=6, random_state=1)\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to use **pandas pipelines to clean** the data since we are only changing one dataframe in these steps. \n",
    "\n",
    "When we build we actually need to consider the split of `X` (the independent features) and `y` (the target feature). Therefore when we're building the models we will just use the standard syntax when using functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y\n",
    "X, y = bikes_processed.pipe(get_Xy)\n",
    "\n",
    "# train models\n",
    "lm = train_model(X, y, linear_model)\n",
    "rf = train_model(X, y, forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the r2 scoring metric, which scores a model between 0.0 - 1.0. 1.0 is the best score a model can achieve, so Linear Regression in this case is doing a lot worse than the Random Forest regressor. But both models could use some improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='date'></a>\n",
    "\n",
    "### Engineered features: date features\n",
    "Feature engineering is the practice of adding features to your data based on what you think might provide additonal information. Let's think about this problem: our dataset concerns bike rentals. One might imagine the number of bikes rented may be correlated to the days people work. Thinking about the dates, there are a couple of features of interest that we might want to add to our dataset:\n",
    "\n",
    "* Is a day a holiday (pre-defined set of dates)? \n",
    "* What is the day of the week (Monday, Tuesday, etc.)?\n",
    "* What season is it?\n",
    "\n",
    "### <mark>Exercise: Pull in the holidays!</mark>\n",
    "\n",
    "Create a new variable `HOLIDAYS` using the USFederalHolidayCalendar() object from pandas. Here is the syntax:\n",
    "\n",
    "```python\n",
    "USFederalHolidayCalendar().holidays(start=, end=)\n",
    "```\n",
    "\n",
    "<mark>**Question**: How many US holidays were there from 2011-2012?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/load-holidays.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some function that will create new features as per the questions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_holiday(df):\n",
    "    \"\"\"Return a new column is_holiday\n",
    "    Input: dataframe (df) with date column (default datetime)\n",
    "    True when the date is a holiday \n",
    "    False when the date is not a holiday\"\"\"\n",
    "    \n",
    "    return df.assign(is_holiday = df.index.isin(HOLIDAYS))\n",
    "\n",
    "def get_weekday(df):\n",
    "    \"\"\"Get the day of the week\"\"\"\n",
    "    \n",
    "    return df.assign(**{'weekday': df.index.day_name()})\n",
    "\n",
    "\n",
    "def get_season(df):\n",
    "    \"\"\"Return the season based off:\n",
    "    Dec, Jan, Feb = winter\n",
    "    Mar, Apr, May = spring\n",
    "    Jun, Jul, Aug = summer\n",
    "    Sep, Oct, Nov = autumn\"\"\"\n",
    "    \n",
    "    season_mapping = {4: 'winter',\n",
    "                      1: 'spring',\n",
    "                      2: 'summer',\n",
    "                      3: 'autumn'}\n",
    "    \n",
    "    offset_months = df.index - pd.DateOffset(months=1)\n",
    "    seasons = offset_months.quarter.map(season_mapping)\n",
    "    \n",
    "    return df.assign(season = seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise: Feature generation</mark>\n",
    "\n",
    "**Create two functions that will add features which you can use to answer:**\n",
    "\n",
    "- <mark>How many weekend days are there in total? </mark>\n",
    "- <mark>How many work days are there in total?</mark>\n",
    "\n",
    "1. Create a boolean column called `is_weekday` where True is a weekday (Mon-Fri) and False a weekend day (Sat-Sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a boolean column called `is_workday` where True is a WORK day (Mon-Fri but NOT a bank holiday) and False a weekend day or Bank Holiday (Sat-Sun and other bank holiday days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/feature-generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new dataframe with the new features\n",
    "\n",
    "We create a new dataframe which has not only undergone preprocessing, but now has also been enriched with the features from dates described above.\n",
    "\n",
    "***Don't forget to add your new features into the pipe-line!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_date_features = (\n",
    "    bikes_processed\n",
    "    .pipe(is_holiday)\n",
    "    .pipe(get_season)\n",
    "    .pipe(get_weekday)\n",
    "    ## Add new functions here ##\n",
    "\n",
    "\n",
    ")\n",
    "bikes_date_features.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we retrain the model on our new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y\n",
    "X, y = get_Xy(bikes_date_features, onehotencoding = ['weekday', 'season'])\n",
    "\n",
    "# train models\n",
    "lm = train_model(X, y, linear_model)\n",
    "rf = train_model(X, y, forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we haven't got much improvement, a small uplift in the Random Forest suggests that there is the potential to add some power here. How about we take this a little bit further..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='past'></a>\n",
    "\n",
    "### Engineered features: past behaviour\n",
    "Another assumption we can make - and try out - is that behaviour in the past at similar time points might be predictive of behaviour of future behaviour. This leads us to a couple of other features to add: \n",
    "* How many bikes were rented in the past 12 hours? \n",
    "* How many bikes were rented in the past 12 days at the exact same hour (e.g. 3pm)? \n",
    "* How many bikes were rented at the same day (e.g. Tuesdays) and same hour (e.g. 9am) in the past 12 weeks? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_date_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enrich our previously created dataframe that has been processed and had date-related features added with our new features and see how this influences model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first(df, col_nam, timedelta):\n",
    "    return (df\n",
    "            .assign(**{col_nam: \n",
    "                       lambda df: np.where(df.index >= (df.index.min() + pd.Timedelta(timedelta)),\n",
    "                                           df[col_nam], \n",
    "                                           np.nan)\n",
    "                      })\n",
    "           )\n",
    "\n",
    "\n",
    "def get_mean_of_previous(df, col_nam, timedelta, grouper=None, \n",
    "                         prev_col='cnt', agg='mean'):\n",
    "    \n",
    "    if grouper is None:\n",
    "        grouper = 'NoGroup'\n",
    "        level_to_drop = 0\n",
    "    else:\n",
    "        level_to_drop = list(range(len(grouper)))\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        .assign(**{col_nam: (lambda df: df\n",
    "                                   .sort_index()\n",
    "                                   .assign(NoGroup=1)\n",
    "                                   .groupby(grouper)\n",
    "                                   .rolling(timedelta, closed='left')\n",
    "                                   [prev_col].agg(agg)\n",
    "                                   .droplevel(level_to_drop)\n",
    "                                  )})\n",
    "        .pipe(remove_first, col_nam, timedelta)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_inferred = (\n",
    "    bikes_date_features\n",
    "    .pipe(get_mean_of_previous, col_nam='last_12_hours', timedelta='12H')\n",
    "    .pipe(get_mean_of_previous, col_nam='last_12_days', timedelta='12D', grouper=['hour'])\n",
    "    .pipe(get_mean_of_previous, col_nam='last_12_weeks', timedelta=f'{12*7}D', grouper=['hour', 'weekday'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exericse: Rebuild the model</mark>\n",
    "Use the functions: `get_Xy` and `train_model`. **Note:** Scikit Learn will not allow `season` or `weekday`. Pass these in as the parameter `onehotencoding=` in the function `get_Xy()`.\n",
    "\n",
    "<mark>**Question:** What is the performance on the train data for the Random Forest Regressor?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y\n",
    "\n",
    "\n",
    "# train models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model: LinearRegression\n",
      "Train score: 0.82\n",
      "Test score: 0.8\n",
      "\n",
      "Building model: RandomForestRegressor\n",
      "Train score: 0.89\n",
      "Test score: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load answers/rebuild-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interestingly enough, the linear regression model seems to have improved a lot more than our Random Forest Regressor. Our random forest regressor seems to have undergone little change with our new features, even though adding our date features greatly improved its performance. Our Linear Regression model, however, seemed to benefit little from our extra date features but thrives with our past behaviour based features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Caveat:** with features that use past values, we need to ensure we are always going to have those past values available. I.e. if we look at the last 12 hours, we need to have the data from one hour ago, which isn't going to be possible if we are trying to predict the `cnt` for the next 3 months._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='encoding'></a>\n",
    "\n",
    "### Engineering features: mean encoding\n",
    "\n",
    "During Feature Engineering the task of converting categorical features into numerical is called Encoding. So far we have just used _one hot encoding_. Let's look at _mean encoding_:\n",
    "\n",
    "Instead of just creating some dummy columns for `season`, let's look at what the average `cnt` is for each season in our `train data` and use that in the model. \n",
    "\n",
    "We have to be really careful here to create our mapping from the train data, as we will not be able to find the `cnt` from our production data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_encodings(df, rand_state=1, grouper='season', columns = ('cnt',)):\n",
    "\n",
    "    \"\"\"Gets mean of columns by grouper. \n",
    "    Default:\n",
    "        grouper='season' \n",
    "        columns='cnt'\n",
    "    \n",
    "    Only uses the X_train data to get the encodings mappings \n",
    "    as these are what would be added to any future data.\"\"\"\n",
    "    \n",
    "    X, y = get_Xy(df)\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X, y, random_state = rand_state)\n",
    "    \n",
    "    mapping = (X_train.join(y_train)\n",
    "               .groupby(grouper)\n",
    "               .agg(**{f'avg_{grouper}_{col}': pd.NamedAgg(col, 'mean') for col in columns})\n",
    "              )\n",
    "    \n",
    "    return pd.merge(df, mapping, \n",
    "                    left_on=grouper, \n",
    "                    right_on=mapping.index, \n",
    "                    right_index=True).drop(grouper, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bikes_encodings = bikes_inferred.pipe(get_mean_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X and y\n",
    "X, y = get_Xy(bikes_encodings, onehotencoding = ['weekday'])\n",
    "\n",
    "# train models\n",
    "lm = train_model(X, y, linear_model)\n",
    "rf = train_model(X, y, forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have improved on both our models, especially our Random Forest by lessening the gap between our Train and Test. This suggests that we are no longer overly fit to our train data. (Overfitting means we have done analysis that it corresponds too much to a particular dataset and can not perform well on unseen data).\n",
    "\n",
    "**Pros of mean encoding:**\n",
    "\n",
    "- Capture information within the label, therefore rendering more predictive features\n",
    "- Creates a monotonic relationship between the variable and the target\n",
    "\n",
    "**Cons of mean encoding:**\n",
    "\n",
    "- It may cause over-fitting in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance \n",
    "\n",
    "From here it would be good to start looking at Feature Selection and see if we can actually reduce amount of features we use. \n",
    "\n",
    "_I know, weird! We just made loads now we want to reduce? Well the model will become more simple with fewer features and we might find that there are some that just don't improve the predictive power of the model!_\n",
    "\n",
    "Let's take a quick look at feature importance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.Series(rf.named_steps['model'].feature_importances_, \n",
    "              X.columns)\n",
    "    .sort_values()\n",
    "\n",
    "    .plot(kind='barh', figsize=(12,10), title='Feature Importance')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that the top 11 features (up until `hum`) are the most important. We could use this to select just our top 10 features and see if that improves the model! But that's for the next chapter: Feature Selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='types'></a>\n",
    "\n",
    "## Types of feature engineering\n",
    "\n",
    "Feature engineering is a way to isolate an highlight key information based on your own domain knowledge, which helps the algorithm focus on what's imporrtant. This is by far not an exhaustive list of the types of feature engineering that exist and the various \n",
    "* **Date and time features** Creating features from the dates available, e.g. is a holidays or day of the week. \n",
    "* **Group values** Grouping various numeric elements to a categorical variable, e.g. the months December (12), January (1) and February (2) to the season Winter. \n",
    "* **Grouping sparse classes** If you have a feature with an individual low sample count, you might group various values together under some other category. For example: if we had a column `bike_type` it would make sense to have stand-alone values such as `race`, `road` or `grandma`, whereas you might want to group values lik `penny farthing`, `unicycle` and `tricycle` together under a single `other` category since they are rarely rented.\n",
    "* **Group from threshold** A new grouped variable for other variables, e.g. `warm` and `cold` based on the temperature.\n",
    "* **Indicator from threshold** An indicator variable (0 or 1) based on a threshold on a column, e.g. eligible to vote based on age. \n",
    "* **Interaction of variables** The sum, difference, product or quotient of two features. E.g. `profit` as result of the difference between income and expenses. \n",
    "\n",
    "#### Adding external data\n",
    "Part of feature engineering can also be to bring in external data. For example, let's say you're given a dataset for predicting house prices which contains the street address and city, and information about the house itself (e.g. number of rooms). Encoding the address information to longtitude and latitude by utilising a geocoding API allows you to bring in information from other external data sources about the neighborhood which might be predictive of the house price, such average income in the neighborhood, number of schools nearby, crime rate etc. \n",
    "\n",
    "#### Important\n",
    "The goal of predictive modelling is often to create a model that can be used in practice. Although nothing is _technically_ holding us back from creating features based on future data (e.g. bike rental in the next 12 hours), in production, this would most likely not be available knowledge. In creating features, keep in mind what situation the data will be gathered and the model used. E.g. if the model to predict the number of bikes that will be rented will be used to allocate a number of bikes to rental locations which is used once a week will not have the number of bikes rented throughout that week available yet (and, for that matter, weather information). So make sure your model is not dependent on features not available when the model is used in practice! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Feature engineering is the practice of constructing new features from existing data which may improve your model. The most important things to keep in mind are: \n",
    "* Feature engineering is typically a time-consuming process, but greatly beneficial to model performance\n",
    "* Only create features that you have available at the time of data collection when the model is used in practice\n",
    "* Feature engineering typically requires a lot of _domain knowledge_ to judge which features would be relevant to add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src='images/next.png' align='right' width=500px>\n",
    "<a id='next'></a>\n",
    "\n",
    "# What's next?\n",
    "\n",
    "***In the training:***\n",
    "\n",
    "- Feature Selection\n",
    "- Object Oriented Programming in Scikit Learn - Making your own estimators/transformers\n",
    "- Advanced Python \n",
    "\n",
    "***In the taster:***\n",
    "\n",
    "- Want more? Sign up to our course!\n",
    "    - [Advanced Data Science with Python](https://godatadriven.com/training/advanced-data-science-with-python-training/)\n",
    "- Want something different? Look at what courses we offer!\n",
    "    - [Python for Data Analysts](https://godatadriven.com/training/python-for-data-analysts-training/)\n",
    "    - [Certified Python for Data Science](https://godatadriven.com/training/data-science-python-foundation-training/)\n",
    "    - [And more!](https://godatadriven.com/what-we-do/train/#upcoming)\n",
    "    \n",
    "Interested in our other courses? Download our [Training Guide](https://godatadriven.com/topic/training-brochure/)\"\n",
    "\n",
    "Hope to see you soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Downloading the Notebook:\n",
    "\n",
    "<img src='images/download.png' width='80px' align='left'>\n",
    "\n",
    "If you would like to there is a Download button at the top of the page. This will download the .ipynb\n",
    "\n",
    "If you are not planning to get Anaconda but you want to save the work you've done, got to File -> Download as and choose .html.\n",
    "\n",
    "<img src='images/visit.png' width='65px' align='left'>\n",
    "\n",
    "Alternatively you can click Visit repo at the top to navigate to the github repo where you can download everything as a .zip file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
